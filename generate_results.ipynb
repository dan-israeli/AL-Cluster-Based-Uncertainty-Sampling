{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **DAV Project --- Results Generation**\n"
      ],
      "metadata": {
        "id": "s9R0ntA0FFQW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this notebook, the results showcased in the main paper are generated, and written into text files.\n",
        "\n",
        "**Note:** Since the project code was written on a virtual machine terminal, which does not allow displaying plots, this notebook solely generates the results discussed in the paper and writes them to TXT files, without displaying them. To display the results, please refer to the provided ```display_results.ipynb``` notebook, with the generated TXT files."
      ],
      "metadata": {
        "id": "ysFMFPKeF6ZE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Initialization**"
      ],
      "metadata": {
        "id": "U1YoqXauFe4g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# imports\n",
        "from torchvision import models, datasets, transforms\n",
        "import torch.nn as nn\n",
        "from matplotlib import pyplot as plt\n",
        "from torch.utils.data import Subset\n",
        "import torch.optim as optim\n",
        "from scipy.stats import entropy\n",
        "import torch\n",
        "import zipfile\n",
        "from torch.utils.data import Dataset\n",
        "from abc import ABC\n",
        "from torchvision.models import resnet50, ResNet50_Weights\n",
        "from sklearn.cluster import KMeans\n",
        "from collections import defaultdict\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import scipy\n",
        "import math\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "# constants\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "IMG_SIZE = 224\n",
        "\n",
        "UNCERTAINTY, SIZE = 0, 1\n",
        "UNCERTAINTY, DATASET_IDX = 0, 1\n",
        "UNCERTAINTY, MAX_PRED_PROB = 0, 1\n",
        "\n",
        "# set seeds for reproducibility\n",
        "SEED = 123\n",
        "\n",
        "def set_random_seeds(seed_value):\n",
        "    torch.manual_seed(seed_value)\n",
        "    torch.cuda.manual_seed(seed_value)\n",
        "    torch.cuda.manual_seed_all(seed_value)\n",
        "    np.random.seed(seed_value)\n",
        "    random.seed(seed_value)\n",
        "\n",
        "set_random_seeds(SEED)"
      ],
      "metadata": {
        "id": "rKpW8E6iEtbY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Data and Model Preparation**"
      ],
      "metadata": {
        "id": "BfDAvUcvEv3Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let us define the model discussed in the main paper, as well as the DataLoader generator that we will utilize throughout the experiments:"
      ],
      "metadata": {
        "id": "JRpyMvAo34KJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# CNN classifier class (based on the pre-tranied ResNet50 model)\n",
        "class Model(nn.Module):\n",
        "  def __init__(self, conv_output_dim, embedding_dim, num_labels, resnet50):\n",
        "    super(Model, self).__init__()\n",
        "\n",
        "    # copy all layers of the pre-trained ResNet50 model except the fully connected layer\n",
        "    self.resnet50 = nn.Sequential(*list(resnet50.children())[:-1])\n",
        "\n",
        "    # convert the convolution output into an embedding vector\n",
        "    self.embedding_layer = nn.Linear(conv_output_dim, embedding_dim)\n",
        "    # activation function\n",
        "    self.relu = nn.ReLU()\n",
        "    # convert the embedding vectors into a vector of 'num_labels' length\n",
        "    self.classification_layer = nn.Linear(embedding_dim, num_labels)\n",
        "    self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "  def forward(self, x):\n",
        "    # get convolution output\n",
        "    conv_output = torch.squeeze(self.resnet50(x), (2, 3))\n",
        "    # get embeddings\n",
        "    embeddings = self.embedding_layer(conv_output)\n",
        "    # apply activation function\n",
        "    embeddings_activated = self.relu(embeddings)\n",
        "    # get final layer\n",
        "    final_layer = self.classification_layer(embeddings_activated)\n",
        "    # apply softmax to extract prediction probabilities\n",
        "    pred_probs = self.softmax(final_layer)\n",
        "\n",
        "    return embeddings, pred_probs"
      ],
      "metadata": {
        "id": "qJ3zlbY033JE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_dataloader(indices, dataset, batch_size=32, shuffle=True):\n",
        "  \"\"\"\n",
        "  Gets indices of specific dataset images.\n",
        "  Returns a dataloader object of the subset dataset (according to the indices).\n",
        "  \"\"\"\n",
        "  sub_dataset = Subset(dataset, indices)\n",
        "  dataloader = torch.utils.data.DataLoader(sub_dataset, batch_size=batch_size, shuffle=shuffle)\n",
        "\n",
        "  return dataloader"
      ],
      "metadata": {
        "id": "lyK-ynd_4CJ8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Cluster Uncertainty Sampling**"
      ],
      "metadata": {
        "id": "3aqOSYXa4EBc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we will define helper functions that will be used when implementing our suggested sampling method:"
      ],
      "metadata": {
        "id": "uj1M-rAQ4O5i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_embeddings_and_pred_probs(indices, dataset, trained_model):\n",
        "  \"\"\"\n",
        "  Gets indices of specific dataset images, and a tranied model.\n",
        "  Returns the embeddings and prediction probabilities of those instances.\n",
        "  \"\"\"\n",
        "  # create dataloader\n",
        "  dataloader = get_dataloader(indices, dataset, shuffle=False)\n",
        "\n",
        "  embeddings_lst = []\n",
        "  pred_probs = []\n",
        "\n",
        "  # get the embddings and predication probabilites of each batch\n",
        "  for batch_X, _ in dataloader:\n",
        "    batch_X = batch_X.to(DEVICE)\n",
        "    batch_embeddings, batch_pred_probs = trained_model(batch_X)\n",
        "\n",
        "    embeddings_lst.append(batch_embeddings)\n",
        "    pred_probs.append(batch_pred_probs)\n",
        "\n",
        "  # concatenate all batch results as a single list\n",
        "  embeddings_lst = torch.cat(embeddings_lst)\n",
        "  pred_probs = torch.cat(pred_probs)\n",
        "\n",
        "  return embeddings_lst.cpu().detach().numpy(), pred_probs.cpu().detach().numpy()\n",
        "\n",
        "\n",
        "def get_sample_clusters(embeddings, num_clusters):\n",
        "  \"\"\"\n",
        "  Gets images embeddings and cluster them using the KMeans algorithm.\n",
        "  Returns the cluster labels of the images\n",
        "  \"\"\"\n",
        "  kmeans = KMeans(n_clusters=num_clusters, n_init='auto', random_state=SEED).fit(embeddings)\n",
        "  return kmeans.labels_\n",
        "\n",
        "\n",
        "def get_cluster_sizes(sample_clusters, cluster_num):\n",
        "  \"\"\"\n",
        "  Gets samples cluster labels of images.\n",
        "  Returns a list of the the cluster sizes.\n",
        "  \"\"\"\n",
        "  cluster_sizes = [0] * cluster_num\n",
        "  for cluster in sample_clusters:\n",
        "    cluster_sizes[cluster] += 1\n",
        "\n",
        "  return cluster_sizes\n",
        "\n",
        "\n",
        "def get_cluster_uncertainty_scores(pred_probs, sample_clusters, cluster_num):\n",
        "  \"\"\"\n",
        "  Get samples predication probabilites.\n",
        "  Returns a list of the cluster uncertainty scores.\n",
        "  \"\"\"\n",
        "  # calculate sample uncertainties using entropy\n",
        "  sample_uncertainties = list(entropy(pred_probs, axis=1))\n",
        "\n",
        "  cluster_members_uncertainties = [[] for i in range(cluster_num)]\n",
        "  for cluster, uncertainty in zip(sample_clusters, sample_uncertainties):\n",
        "    cluster_members_uncertainties[cluster].append(uncertainty)\n",
        "\n",
        "  # calculate each cluster uncertainty score as the mean uncertainties of its members\n",
        "  cluster_uncertainty_scores = [0] * cluster_num\n",
        "  for cluster, cluster_members_uncertainties in enumerate(cluster_members_uncertainties):\n",
        "    cluster_uncertainty_scores[cluster] = np.mean(cluster_members_uncertainties)\n",
        "\n",
        "  return cluster_uncertainty_scores\n",
        "\n",
        "\n",
        "def get_cluster_overall_scores(pred_probs, sample_clusters, cluster_num, alpha):\n",
        "  \"\"\"\n",
        "  Gets samples predication probabilites and cluster labels.\n",
        "  Returns a list of the cluster overall scores:\n",
        "  weighting (using alpha) of the cluster uncertainty score and its size.\n",
        "  \"\"\"\n",
        "  # get cluster sizes\n",
        "  cluster_sizes = get_cluster_sizes(sample_clusters, cluster_num)\n",
        "  cluster_sizes = np.array(cluster_sizes).reshape(-1, 1)\n",
        "  # get cluster uncertainty scores\n",
        "  cluster_uncertainty_scores = get_cluster_uncertainty_scores(pred_probs, sample_clusters, cluster_num)\n",
        "  cluster_uncertainty_scores = np.array(cluster_uncertainty_scores).reshape(-1, 1)\n",
        "\n",
        "  # scale both metrics to have the same scale (0-1)\n",
        "  scaler = MinMaxScaler()\n",
        "  scaled_cluster_uncertainty_scores = scaler.fit_transform(cluster_uncertainty_scores).reshape(1, -1)[0]\n",
        "  scaled_cluster_sizes = scaler.fit_transform(cluster_sizes).reshape(1, -1)[0]\n",
        "\n",
        "  # calculate each cluster score as a weighted sum of the scaled metrics\n",
        "  cluster_overall_scores = [0] * cluster_num\n",
        "  for cluster, (size, uncertainty_score) in enumerate(zip(scaled_cluster_sizes, scaled_cluster_uncertainty_scores)):\n",
        "    cluster_overall_scores[cluster] = alpha * size + (1 - alpha) * uncertainty_score\n",
        "\n",
        "  return cluster_overall_scores\n",
        "\n",
        "\n",
        "def assign_remaining_budget(cluster_budgets, cluster_sizes, cluster_overall_scores, remaining_budget):\n",
        "  \"\"\"\n",
        "  Assigns unallocated budget to clusters based on their overall scores (from highest to lowest)\n",
        "  \"\"\"\n",
        "  # sort the cluster labels based on their scores in descending order\n",
        "  highest_score_clusters = np.argsort(-np.array(cluster_overall_scores))\n",
        "\n",
        "  for cluster in highest_score_clusters:\n",
        "    cluster_budget, cluster_size = cluster_budgets[cluster], cluster_sizes[cluster]\n",
        "\n",
        "    # check if the cluster budget could be increased\n",
        "    if cluster_budget < cluster_size:\n",
        "\n",
        "      # calculate the remaining sample amount of the cluster\n",
        "      cluster_remaining_sample_amount = cluster_size - cluster_budget\n",
        "\n",
        "      # get the increased budget delta as the minimum of the following:\n",
        "      # - the remaining sample amount of the cluster\n",
        "      # - the remaining_budget\n",
        "      cluster_increased_budget_delta = min(cluster_remaining_sample_amount, remaining_budget)\n",
        "\n",
        "      # increase the cluster budget\n",
        "      cluster_budgets[cluster] += cluster_increased_budget_delta\n",
        "\n",
        "      # update remaining unallocated budget for this iteration\n",
        "      remaining_budget -= cluster_increased_budget_delta\n",
        "\n",
        "      # stop the procedure when all the unallocated budget has been assigend\n",
        "      if remaining_budget == 0:\n",
        "        break\n",
        "\n",
        "\n",
        "def get_cluster_budgets(pred_probs, sample_clusters, alpha, n_select):\n",
        "  \"\"\"\n",
        "  Gets samples prediction probabilites and cluster labels.\n",
        "  Returns the sampling budget of each cluster (proportional to 'n_select').\n",
        "  \"\"\"\n",
        "  # calculate the number of clusters\n",
        "  cluster_num = len(set(sample_clusters))\n",
        "\n",
        "  # get cluster sizes\n",
        "  cluster_sizes = get_cluster_sizes(sample_clusters, cluster_num)\n",
        "\n",
        "  # get cluster overall scores\n",
        "  cluster_overall_scores = get_cluster_overall_scores(pred_probs, sample_clusters, cluster_num, alpha)\n",
        "  # convert cluster scores into sampling proportions using softmax\n",
        "  cluster_relative_scores = scipy.special.softmax(cluster_overall_scores)\n",
        "\n",
        "  # calculate each cluster sampling budget as the minimum of the following:\n",
        "  # proportion of the current iteration budget ('n_select') w.r.t the realtive score\n",
        "  # the total number of members\n",
        "  cluster_budgets = []\n",
        "  for cluster_relative_score, cluster_size in zip(cluster_relative_scores, cluster_sizes):\n",
        "    cluster_budget = min(math.floor(cluster_relative_score * n_select), cluster_size)\n",
        "    cluster_budgets.append(cluster_budget)\n",
        "\n",
        "  # calculate the remaining budget\n",
        "  remaining_budget = n_select - sum(cluster_budgets)\n",
        "\n",
        "  # assign remaining budget\n",
        "  if remaining_budget > 0:\n",
        "    assign_remaining_budget(cluster_budgets, cluster_sizes, cluster_overall_scores, remaining_budget)\n",
        "\n",
        "  return cluster_budgets\n",
        "\n",
        "\n",
        "def get_cluster_members(pred_probs, sample_clusters, dataset_indices):\n",
        "  \"\"\"\n",
        "  Gets samples predication probabilites and cluster labels.\n",
        "  Return a dictionary of the form:\n",
        "  key - cluster label\n",
        "  value - list of cluster member tuples of the form (member uncertainty, member dataset index)\n",
        "  \"\"\"\n",
        "  # calculate sample uncertainties using entropy\n",
        "  uncertainties = list(entropy(pred_probs, axis=1))\n",
        "\n",
        "  # get each cluster member tuples\n",
        "  cluster_members_dict = defaultdict(list)\n",
        "  for cluster, uncertainty, dataset_idx in zip(sample_clusters, uncertainties, dataset_indices):\n",
        "    cluster_members_dict[cluster].append((uncertainty, dataset_idx))\n",
        "\n",
        "  return cluster_members_dict"
      ],
      "metadata": {
        "id": "lzDTtR8a4mw5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, let us implement the Cluster Uncertainty sampling method below:"
      ],
      "metadata": {
        "id": "dgUE8nWb4s13"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def cluster_uncertainty_sampling(available_pool_indices, dataset, n_select, trained_model, num_clusters, alpha):\n",
        "  \"\"\"\n",
        "  Sample indices from the availabel pool as follows:\n",
        "  1. cluster the images based on thier embeddings\n",
        "  2. calculate each cluster's score as a weighting of its uncertainty score and size\n",
        "  3. calculate each cluster's sampling budget as its relative score times 'n_select'\n",
        "  4. sample from each cluster the instances with the highest uncertainty based on its budget\n",
        "  \"\"\"\n",
        "  # get samples embeddings and prediction probabilities\n",
        "  embeddings, pred_probs = get_embeddings_and_pred_probs(available_pool_indices, dataset, trained_model)\n",
        "\n",
        "  # cluster samples based on their embeddings\n",
        "  sample_clusters = get_sample_clusters(embeddings, num_clusters)\n",
        "\n",
        "  # get each cluster's members\n",
        "  cluster_members = get_cluster_members(pred_probs, sample_clusters, available_pool_indices)\n",
        "\n",
        "  # get each cluster's budget\n",
        "  cluster_budgets = get_cluster_budgets(pred_probs, sample_clusters, alpha, n_select)\n",
        "\n",
        "  # sample from each cluster based on its budget\n",
        "  selected_indices = []\n",
        "  for cluster, cluster_budget in enumerate(cluster_budgets):\n",
        "    cluster_members_uncertainties = cluster_members[cluster]\n",
        "\n",
        "    # sample the instances with the highest uncertainties\n",
        "    selected_cluster_members = sorted(cluster_members_uncertainties, key=lambda x: x[UNCERTAINTY], reverse=True)[:cluster_budget]\n",
        "    selected_cluster_indices = [tup[DATASET_IDX] for tup in selected_cluster_members]\n",
        "\n",
        "    selected_indices += selected_cluster_indices\n",
        "\n",
        "  return selected_indices"
      ],
      "metadata": {
        "id": "RMSpAGrK4ypj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Baseline Sampling Methods**\n"
      ],
      "metadata": {
        "id": "aQZpz_Ay45h2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let us implement the baseline sampling methods discussed in the main paper:\n",
        "\n",
        "- Random Sampling\n",
        "\n",
        "- Entropy Uncertainty Samplng\n",
        "\n",
        "- MinMax Uncertainty Sampling"
      ],
      "metadata": {
        "id": "4xirXxlT89ot"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def random_sampling(available_pool_indices, n_select):\n",
        "  \"\"\"\n",
        "  Randomly sample indices from the available pool.\n",
        "  \"\"\"\n",
        "  # randomly select n_select indices without replacement\n",
        "  selected_indices = list(np.random.choice(available_pool_indices, n_select, replace=False))\n",
        "\n",
        "  return selected_indices\n",
        "\n",
        "\n",
        "def entropy_uncertainty_sampling(available_pool_indices, dataset, n_select, trained_model):\n",
        "  \"\"\"\n",
        "  Sample indices from the availabel pool with the highest uncertainty.\n",
        "  The uncertainty under this method is measured as follows:\n",
        "  1. Calculate each example's entropy of the model's predication probabilities\n",
        "  2. Choose the \"n_select\" examples with the highest entropy\n",
        "  \"\"\"\n",
        "\n",
        "  # get prediction probabilites for unlabeled samples\n",
        "  _, pred_probs = get_embeddings_and_pred_probs(available_pool_indices, dataset, trained_model)\n",
        "\n",
        "  # calculate sample uncertainties using entropy\n",
        "  uncertainties = list(entropy(pred_probs, axis=1))\n",
        "  uncert_ind_tups = zip(uncertainties, available_pool_indices)\n",
        "\n",
        "  # select samples with highest uncertaintt (entropy)\n",
        "  selected_uncert_ind_tups = sorted(uncert_ind_tups, key=lambda x: x[UNCERTAINTY], reverse=True)[:n_select]\n",
        "  selected_indices = [tup[DATASET_IDX] for tup in selected_uncert_ind_tups]\n",
        "\n",
        "  return selected_indices\n",
        "\n",
        "\n",
        "def minmax_uncertainty_sampling(available_pool_indices, dataset, n_select, trained_model):\n",
        "  \"\"\"\n",
        "  Sample indices from the availabel pool with the highest uncertainty.\n",
        "  The uncertainty under this method is measured as follows:\n",
        "  1. For each example, find the maxiumum probability among the model's predication distribution,\n",
        "  2. Choose the \"n_select\" examples with the lowest (minimum) maximum probability\n",
        "  \"\"\"\n",
        "  # get prediction probabilites for unlabeled samples\n",
        "  _, pred_probs = get_embeddings_and_pred_probs(available_pool_indices, dataset, trained_model)\n",
        "\n",
        "  # calculate sample max predication probabilities\n",
        "  max_pred_probs = list(pred_probs.max(axis=1))\n",
        "  max_pred_prob_ind_tups = zip(max_pred_probs, available_pool_indices)\n",
        "\n",
        "  # select samples with lowest max predication probabilities\n",
        "  selected_max_pred_prob_ind_tups = sorted(max_pred_prob_ind_tups, key=lambda x: x[MAX_PRED_PROB])[:n_select]\n",
        "  selected_indices = [tup[DATASET_IDX] for tup in selected_max_pred_prob_ind_tups]\n",
        "\n",
        "  return selected_indices"
      ],
      "metadata": {
        "id": "rpFsabdJ9Kwz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Active Learning Pipeline**"
      ],
      "metadata": {
        "id": "ATiPJwhxqsRL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let us define functions that will be used during the AL pipeline run:"
      ],
      "metadata": {
        "id": "kR8D-gsbGR0t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sample_indices(sampling_method, available_pool_indices, dataset, n_select, trained_model, num_clusters, alpha):\n",
        "  \"\"\"\n",
        "  Gets a sampling method and its respective parameters.\n",
        "  Returns the selected indices accordingly.\n",
        "  \"\"\"\n",
        "  if sampling_method == \"random\":\n",
        "    selected_indices = random_sampling(available_pool_indices, n_select)\n",
        "\n",
        "  elif sampling_method == \"entropy uncertainty\":\n",
        "    selected_indices = entropy_uncertainty_sampling(available_pool_indices, dataset, n_select, trained_model)\n",
        "\n",
        "  elif sampling_method == \"minmax uncertainty\":\n",
        "    selected_indices = minmax_uncertainty_sampling(available_pool_indices, dataset, n_select, trained_model)\n",
        "\n",
        "  elif sampling_method == \"cluster uncertainty\":\n",
        "    selected_indices = cluster_uncertainty_sampling(available_pool_indices, dataset, n_select, trained_model, num_clusters, alpha)\n",
        "\n",
        "  return selected_indices\n",
        "\n",
        "def init_model(input_dim, embedding_dim, output_dim):\n",
        "  \"\"\"\n",
        "  Gets dimensions for the different layers of the model.\n",
        "  Returns the initialized model.\n",
        "  \"\"\"\n",
        "  # initialize the pre-trained ResNet50 model\n",
        "  resnet50 = models.resnet50(weights=ResNet50_Weights.DEFAULT).to(DEVICE)\n",
        "\n",
        "  # initialize the model\n",
        "  model = Model(input_dim, embedding_dim, output_dim, resnet50).to(DEVICE)\n",
        "\n",
        "  # freeze convolutional layers parameters\n",
        "  for param in model.resnet50.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "  return model\n",
        "\n",
        "\n",
        "def train_model(model, train_indices, dataset, epochs):\n",
        "  \"\"\"\n",
        "  Trains the model on the train instances of the dataset.\n",
        "  \"\"\"\n",
        "  # initialize optimizer\n",
        "  optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "  # initialize loss function\n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "  # create dataloader\n",
        "  dataloader = get_dataloader(train_indices, dataset)\n",
        "\n",
        "  # train the model the the train set\n",
        "  for epoch in range(epochs):\n",
        "    for train_X, train_y in dataloader:\n",
        "      train_X, train_y = train_X.to(DEVICE), train_y.to(DEVICE)\n",
        "      _, pred_probs = model(train_X)\n",
        "\n",
        "      pred_probs = pred_probs.to(DEVICE)\n",
        "      loss = criterion(pred_probs, train_y)\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "  return model\n",
        "\n",
        "\n",
        "def evaluate_model_accuracy(trained_model, test_indices, dataset):\n",
        "  \"\"\"\n",
        "  Evaluates the trained model on the test instances of the dataset (using accuracy).\n",
        "  \"\"\"\n",
        "  # set model to evaluation model\n",
        "  trained_model.eval()\n",
        "  # create dataloader\n",
        "  dataloader = get_dataloader(test_indices, dataset, shuffle=False)\n",
        "\n",
        "  # calculate the trained model accuracy score on the test set\n",
        "  summ = 0\n",
        "  for test_X, test_y in dataloader:\n",
        "    test_X, test_y = test_X.to(DEVICE), test_y.to(DEVICE)\n",
        "    _, pred_probs = trained_model(test_X)\n",
        "\n",
        "    pred_probs = pred_probs.to(DEVICE)\n",
        "    pred_labels = torch.argmax(pred_probs, dim=1)\n",
        "\n",
        "    summ += torch.sum(pred_labels == test_y)\n",
        "\n",
        "  accuracy = summ / len(test_indices)\n",
        "  return accuracy.item()\n",
        "\n",
        "\n",
        "def performance_analysis(trained_model, test_indices, dataset, iter, sampling_method):\n",
        "  \"\"\"\n",
        "  Analyzes the trained model's performance on the test set in every iteration,\n",
        "  based on several evaluation metrics:\n",
        "\n",
        "  - Confusion matrix items\n",
        "  - TPR and TNR rates\n",
        "  - F1 score\n",
        "\n",
        "  Writes the results to a txt file with the name \"performance_analysis - <sampling_method>.txt\".\n",
        "  \"\"\"\n",
        "  # set model to evaluation model\n",
        "  trained_model.eval()\n",
        "  # create dataloader\n",
        "  dataloader = get_dataloader(test_indices, dataset, shuffle=False)\n",
        "\n",
        "  # count confusion matrix items (TP, FP, TN, FN)\n",
        "  TP, FP, TN, FN = 0, 0, 0, 0\n",
        "  for batch_num, (test_X, test_y) in enumerate(dataloader):\n",
        "    test_X, test_y = test_X.to(DEVICE), test_y.to(DEVICE)\n",
        "    _, pred_probs = trained_model(test_X)\n",
        "\n",
        "    pred_probs = pred_probs.to(DEVICE)\n",
        "    pred_labels = torch.argmax(pred_probs, dim=1)\n",
        "\n",
        "    TP += torch.sum((pred_labels == 1) & (test_y == 1))\n",
        "    FP += torch.sum((pred_labels == 1) & (test_y == 0))\n",
        "    TN += torch.sum((pred_labels == 0) & (test_y == 0))\n",
        "    FN += torch.sum((pred_labels == 0) & (test_y == 1))\n",
        "\n",
        "  # calculate metrics and write to txt file\n",
        "  recall, precision = TP / (TP + FN), TP / (TP + FP)\n",
        "  f1_score = 2 * (precision * recall) / (precision + recall)\n",
        "\n",
        "  with open(f\"performance_analysis - {sampling_method}.txt\", \"a\") as f:\n",
        "    f.write(f\"Evaluation Metrics --- Iteration {iter}:\\n\")\n",
        "    # write confusion matrix items\n",
        "    f.write(f\"TP: {TP} \\nFP: {FP} \\nTN: {TN} \\nFN: {FN} \\n\")\n",
        "    # write TPR and TNR rates\n",
        "    f.write(f\"TPR: {TP / (TP + FN)} \\nTNR: {TN / (TN + FP)} \\n\")\n",
        "    # write the F1 score\n",
        "    f.write(f\"F1 Score: {f1_score:.4f}\\n\\n\")"
      ],
      "metadata": {
        "id": "vhwNMCL6GPie"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lastly, let us implement the active learning pipeline:"
      ],
      "metadata": {
        "id": "3MCheFnTGa4_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def AL_pipeline(train_indices, available_pool_indices, test_indices,\n",
        "                budget_per_iter, total_budget, num_iters, epochs,\n",
        "                dataset, sampling_method, num_clusters=None, alpha=None,\n",
        "                conv_output_dim=2048, embedding_dim=1024, num_labels=2,\n",
        "                analyze_performance=False):\n",
        "  \"\"\"\n",
        "  Gets the AL pipeline parameters, and runs it accordingly.\n",
        "  Returns the accuracy scores of the model at each iteration.\n",
        "  \"\"\"\n",
        "  accuracy_scores = []\n",
        "  budget_left = total_budget\n",
        "  for iter in range(num_iters):\n",
        "    print()\n",
        "    print(f\"Iteration: {iter}\")\n",
        "\n",
        "    # initialize the model\n",
        "    model = init_model(conv_output_dim, embedding_dim, num_labels)\n",
        "\n",
        "    # train the model (on the train set)\n",
        "    trained_model = train_model(model, train_indices, dataset, epochs)\n",
        "\n",
        "    # calculate test set accuracy\n",
        "    accuracy = evaluate_model_accuracy(trained_model, test_indices, dataset)\n",
        "    accuracy_scores.append(round(accuracy, 4))\n",
        "\n",
        "    # analyze performance if needed\n",
        "    if analyze_performance:\n",
        "      performance_analysis(trained_model, test_indices, dataset, iter, sampling_method)\n",
        "\n",
        "    # find the number of instances to add to the train set\n",
        "    n_select = min(budget_per_iter, len(available_pool_indices))\n",
        "    budget_left -= n_select\n",
        "\n",
        "    # check if the budget has been reached\n",
        "    if n_select == 0:\n",
        "      break\n",
        "\n",
        "    # select instances to label based on the choosen sampling method\n",
        "    selected_indices = sample_indices(sampling_method, available_pool_indices, dataset, n_select,\n",
        "                                      trained_model, num_clusters, alpha)\n",
        "\n",
        "    # add selected indices to the train set\n",
        "    train_indices += selected_indices\n",
        "\n",
        "    # remove the new added train indices from the available pool\n",
        "    available_pool_indices = list(set(available_pool_indices) - set(selected_indices))\n",
        "\n",
        "  return accuracy_scores"
      ],
      "metadata": {
        "id": "APpNB_GMqhgT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Experiments**"
      ],
      "metadata": {
        "id": "gtv9dr9eqjPe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this section, we will run all experiments discussed in the main paper, generate the results, and write them to TXT files."
      ],
      "metadata": {
        "id": "uY_2x4S4q3az"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "First, please upload the ZIP file which can be found in the following [link](https://drive.google.com/file/d/1fJumk1nYybXVXSbzHVo8oATe4X57NVbs/view?usp=drive_link), **to the same directory as this notebook**, and run the following cell. If you don't, please change the path for the ```zip_dir``` variable below, to the correct path of the ZIP file.\n"
      ],
      "metadata": {
        "id": "ohkyjJlIrTkb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "zip_dir = 'data.zip'"
      ],
      "metadata": {
        "id": "o5WT_P-ssLRs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then, unzip the file:"
      ],
      "metadata": {
        "id": "zKm_1A7LkqjX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with zipfile.ZipFile(zip_dir, 'r') as zip_ref:\n",
        " zip_ref.extractall()"
      ],
      "metadata": {
        "id": "zHL7zYZjkUMC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, if the 'data' directory created after unzipping 'data.zip' is not in the same directory as this notebook, please move it there. If you don't, please change the path for the ```data_dir``` variable below, to the correct 'data' directory path."
      ],
      "metadata": {
        "id": "jqAM6GCasSvg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_dir = 'data'"
      ],
      "metadata": {
        "id": "L61kLAoPk86c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that the data is set, let us process it:"
      ],
      "metadata": {
        "id": "ecWT6InKlCCW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# transform the dataset and load into a Dataset object\n",
        "transformer = transforms.Compose(\n",
        "              [transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
        "                transforms.ToTensor()])\n",
        "\n",
        "dataset = datasets.ImageFolder(data_dir, transformer)\n",
        "\n",
        "# randomize the instances\n",
        "randomized_indices = torch.randperm(len(dataset)).tolist()\n",
        "dataset = Subset(dataset, randomized_indices)"
      ],
      "metadata": {
        "id": "QNc_m4j1svd5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we will define the following helper function before conducting the experiments:"
      ],
      "metadata": {
        "id": "KRf9N34muGiD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def split_data_indices(data_size):\n",
        "  \"\"\"\n",
        "  Splits the data into train, test, and available pool indices, as discussed\n",
        "  in the main paper.\n",
        "  \"\"\"\n",
        "  data_size = len(dataset)\n",
        "  train_indices = list(range(0, int(data_size*0.2)))\n",
        "  test_indices = list(range(int(data_size*0.2), int(data_size*0.4)))\n",
        "  available_pool_indices = list(range(int(data_size*0.4), int(data_size)))\n",
        "\n",
        "  return train_indices, test_indices, available_pool_indices"
      ],
      "metadata": {
        "id": "gFp92z7tuKDw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Hyperparameter Tuning**"
      ],
      "metadata": {
        "id": "o7ijxkx2vjpx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let us perform grid search in order to optimize for the best (k, $\\alpha$) hyperparameters, with the embedding size $D$ set constant at 1024, as discussed in the paper:"
      ],
      "metadata": {
        "id": "wCxXOCl3vqoi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def grid_search(dataset, k_values, alpha_values):\n",
        "  \"\"\"\n",
        "  Performs grid search in order to optimize for the best (k, alpha) hyperparameters.\n",
        "  Writes the accuracy results to a TXT file named \"combination_accuracies.txt\".\n",
        "  \"\"\"\n",
        "  for k in k_values:\n",
        "    for alpha in alpha_values:\n",
        "      print(f\"----- k={k}, alpha={alpha} -----\")\n",
        "\n",
        "      # reset random seed\n",
        "      set_random_seeds(SEED)\n",
        "\n",
        "      # split to indices\n",
        "      train_indices, test_indices, available_pool_indices = split_data_indices(len(dataset))\n",
        "\n",
        "      # run AL pipeline\n",
        "      num_iters = 10\n",
        "      total_budget = len(available_pool_indices)  # total number of available unlabeled instances\n",
        "      budget_per_iter = math.floor(len(available_pool_indices) / num_iters)  # 10% of the total number\n",
        "\n",
        "      accuracies = AL_pipeline(train_indices, available_pool_indices, test_indices, budget_per_iter=budget_per_iter,\n",
        "             total_budget=total_budget, num_iters=num_iters, epochs=3, dataset=dataset, sampling_method=\"cluster uncertainty\",\n",
        "\t           num_clusters=k, alpha=alpha, embedding_dim=1024)\n",
        "\n",
        "      # write results to TXT file\n",
        "      with open(\"combination_accuracies.txt\", \"a\") as f:\n",
        "        f.write(f\"({k}, {alpha}): {accuracies}\\n\")"
      ],
      "metadata": {
        "id": "gG_xe7IkvbG_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "k_values = [2, 4, 6, 8, 10]\n",
        "alpha_values = [0.0, 0.2, 0.4, 0.6, 0.8, 1.0]\n",
        "\n",
        "grid_search(dataset, k_values, alpha_values)"
      ],
      "metadata": {
        "id": "5O43JfZNwIjR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Note:** The accuracy results are written to a TXT file named ```combination_accuracies.txt```, and are displayed in the provided ```display_results.ipynb``` notebook."
      ],
      "metadata": {
        "id": "o76X-jpZC35N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "After obtaining the results, the best hyperparameter values are k = 6 and $\\alpha$ = 0.4 (can be seen in the ```display_results.ipynb``` file or in the main paper). Therefore, we will proceed with those tuned values, and optimize for the best performing embedding dimension:"
      ],
      "metadata": {
        "id": "5evhSG9GwZp1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "best_k, best_alpha = 6, 0.4"
      ],
      "metadata": {
        "id": "ZoDP5e1pLvcx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def optimize_embedding_dim(dataset, embedding_dim_lst):\n",
        "  \"\"\"\n",
        "  Optimizes for the best embedding dimension.\n",
        "  Writes the accuracy results to a TXT file named \"embedding_dim_accuracies.txt\".\n",
        "  \"\"\"\n",
        "  for embedding_dim in embedding_dim_lst:\n",
        "    print(f\"--- running embedding dim {embedding_dim} ---\")\n",
        "    # reset random seed\n",
        "    set_random_seeds(SEED)\n",
        "\n",
        "    # split to indices\n",
        "    train_indices, test_indices, available_pool_indices = split_data_indices(len(dataset))\n",
        "\n",
        "    # run AL pipeline\n",
        "    num_iters = 10\n",
        "    total_budget = len(available_pool_indices)\n",
        "    budget_per_iter = math.floor(len(available_pool_indices) / num_iters)\n",
        "\n",
        "    accuracies = AL_pipeline(train_indices, available_pool_indices, test_indices, budget_per_iter=budget_per_iter,\n",
        "            total_budget=total_budget, num_iters=num_iters, epochs=3, dataset=dataset, sampling_method=\"cluster uncertainty\",\n",
        "            num_clusters=best_k, alpha=best_alpha, embedding_dim=embedding_dim)\n",
        "\n",
        "    # write results to TXT file\n",
        "    with open(\"embedding_dim_accuracies.txt\", \"a\") as f:\n",
        "      f.write(f\"embedding dim {embedding_dim}: {accuracies}\\n\")"
      ],
      "metadata": {
        "id": "fsP_4aDP3Lhd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_dim_lst = [256, 512, 768]  # embedding dim 1024 was already examined previously\n",
        "optimize_embedding_dim(dataset, embedding_dim_lst)"
      ],
      "metadata": {
        "id": "jKAUT-Bg3R__"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Note:** The accuracy results are written to a TXT file named ```embedding_dim_accuracies.txt```, and are displayed in the provided ```display_results.ipynb``` notebook."
      ],
      "metadata": {
        "id": "ESxskOKcDZqd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "After obtaining the results, the best embedding dimension is $D=1024$ (as can be seen in the ```display_results.ipynb``` file or in the main paper)."
      ],
      "metadata": {
        "id": "ie0y4_cH3bLW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "best_embedding_dim = 1024"
      ],
      "metadata": {
        "id": "p3XCwGsFL3So"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Hyperparameter Tuning Summary**\n",
        "\n",
        "Thus, to summarize, the optimal hyperparameters for our sampling method are:\n",
        "\n",
        "- k = 6\n",
        "\n",
        "- $\\alpha$ = 0.4\n",
        "\n",
        "- $D$ = 1024"
      ],
      "metadata": {
        "id": "IZSTKpIiEYp-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Performance Analysis**"
      ],
      "metadata": {
        "id": "M69J2CBw389r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this section, we analyze our method's performance and compare it to baseline sampling methods:\n",
        "\n",
        "- Random Sampling\n",
        "\n",
        "- Entropy Uncertainty Sampling\n",
        "\n",
        "- MinMax Uncertainty Sampling"
      ],
      "metadata": {
        "id": "afRmTp0R-FWG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sampling_methods = [\"cluster uncertainty\", \"random\", \"entropy uncertainty\", \"minmax uncertainty\"]"
      ],
      "metadata": {
        "id": "wmByel74-X5n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "When calling the AL pipeline function, we will set ```analyze_results=True```, in order to calculate the following calculation metrics and write to a TXT file:\n",
        "\n",
        "- Confusion matrix items (TP, FP, TN, FN)\n",
        "\n",
        "- TPR and TNR rates\n",
        "\n",
        "- F1 score\n",
        "\n",
        "- Accuracy"
      ],
      "metadata": {
        "id": "QlAoMTUuBt6r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compare_sampling_methods(dataset, sampling_methods):\n",
        "  \"\"\"\n",
        "  Compares the performance of the AL pipeline with all baseline sampling methods.\n",
        "  The baselines examined, as well as the evaluation metrics, are detailed in the main paper.\n",
        "  Writes the evaluation metric results to a TXT file named \"performance_analysis - <sampling_method>.txt\",\n",
        "  for each examined sampling method in 'sampling_methods'.\n",
        "  \"\"\"\n",
        "  for sampling_method in sampling_methods:\n",
        "    print(f\"--- running {sampling_method} sampling method ---\")\n",
        "\n",
        "    # reset random seed\n",
        "    set_random_seeds(SEED)\n",
        "\n",
        "    # split to indices\n",
        "    train_indices, test_indices, available_pool_indices = split_data_indices(len(dataset))\n",
        "\n",
        "    # run AL pipeline\n",
        "    num_iters = 10\n",
        "    total_budget = len(available_pool_indices)\n",
        "    budget_per_iter = math.floor(len(available_pool_indices) / num_iters)\n",
        "\n",
        "    if sampling_method == \"cluster uncertainty\":\n",
        "      num_clusters, alpha = best_k, best_alpha\n",
        "    else:\n",
        "      num_clusters, alpha = None, None\n",
        "\n",
        "    accuracies = AL_pipeline(train_indices, available_pool_indices, test_indices, budget_per_iter=budget_per_iter,\n",
        "                total_budget=total_budget, num_iters=num_iters, epochs=3, dataset=dataset, sampling_method=sampling_method,\n",
        "                num_clusters=num_clusters, alpha=alpha, embedding_dim=best_embedding_dim, analyze_performance=True)\n",
        "\n",
        "    # The performance analysis does not cover accuracies across the iterations.\n",
        "    # Thus, let us write it to the appropriate performance analysis TXT file manually:\n",
        "    with open(f\"performance_analysis - {sampling_method}.txt\", \"a\") as f:\n",
        "      f.write(f\"Accuracies across all iterations: {accuracies}\\n\")"
      ],
      "metadata": {
        "id": "tCVtdxbw-fl7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "compare_sampling_methods(dataset, sampling_methods)"
      ],
      "metadata": {
        "id": "iDJD_G0sNZiG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Note:** The evaluation metric results are written to a TXT file named ```performance_analysis - <sampling_method>.txt``` where ```<sampling_method>``` is the appropriate method that was evaluated. Moreover, the results are displayed in the provided ```display_results.ipynb``` notebook."
      ],
      "metadata": {
        "id": "B_bgkE9jDj3e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Generated Results Summary**\n",
        "\n",
        "To summarize, the following files were generated, containing all experiment results:\n",
        "\n",
        "- ```combination_accuracies.txt```\n",
        "\n",
        "- ```embedding_dim_accuracies.txt```\n",
        "\n",
        "- ```performance_analysis - cluster uncertainty.txt```\n",
        "\n",
        "- ```performance_analysis - random.txt```\n",
        "\n",
        "- ```performance_analysis - entropy uncertainty.txt```\n",
        "\n",
        "- ```performance_analysis - minmax uncertainty.txt```\n",
        "\n",
        "\n",
        "As mentioned above, in order to display these results, please refer to the provided ```display_results.ipynb``` notebook."
      ],
      "metadata": {
        "id": "EIhk5LwNE4xo"
      }
    }
  ]
}